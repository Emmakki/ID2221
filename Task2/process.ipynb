{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [emma-Inspiron-3501]\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - emma supergroup          0 2024-10-23 18:56 /user/OBIS/data\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/bin/hadoop fs -ls /user/OBIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/25 22:30:27 WARN Utils: Your hostname, emma-Inspiron-3501 resolves to a loopback address: 127.0.1.1; using 192.168.10.220 instead (on interface wlp0s20f3)\n",
      "24/10/25 22:30:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/emma/.ivy2/cache\n",
      "The jars for the packages stored in: /home/emma/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5ed2b67d-61b8-4695-82c6-a2c0f88cd466;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.3.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      ":: resolution report :: resolve 1659ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.3.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   1   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5ed2b67d-61b8-4695-82c6-a2c0f88cd466\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/8ms)\n",
      "24/10/25 22:30:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def init_spark():\n",
    "    mongo_conn = \"mongodb+srv://obis:obis-project@obis-results.cer3o.mongodb.net/?retryWrites=true&w=majority&appName=OBIS-results\"\n",
    "    conf = SparkConf()\n",
    "\n",
    "    # Download mongo-spark-connector and its dependencies.\n",
    "    conf.set(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\")\n",
    "\n",
    "    # Set up read connection :\n",
    "    conf.set(\"spark.mongodb.read.connection.uri\", mongo_conn)\n",
    "    conf.set(\"spark.mongodb.read.database\", \"biodiversity_db\")\n",
    "\n",
    "    # Set up write connection\n",
    "    conf.set(\"spark.mongodb.write.connection.uri\", mongo_conn)\n",
    "    conf.set(\"spark.mongodb.write.database\", \"biodiversity_db\")\n",
    "    # If you need to update instead of inserting :\n",
    "    conf.set(\"spark.mongodb.write.operationType\", \"update\")\n",
    "\n",
    "    SparkContext(conf=conf)\n",
    "\n",
    "    return SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('OBIS') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hdfs_directory_path = \"hdfs://localhost:9000/user/OBIS/data\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.option(\"header\", \"true\").csv(hdfs_directory_path)\n",
    "except Exception as e:\n",
    "    print(\"Error reading from HDFS:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, countDistinct, count\n",
    "\n",
    "species_count_per_species = df.groupBy(\"scientificname\").agg(count(\"*\").alias(\"count_per_species\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biodiversity hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "cleaned_df = df.filter(col('decimalLatitude').isNotNull() & col('decimalLongitude').isNotNull() & col('scientificName').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "grid_size = 1.0\n",
    "species_richness_df = cleaned_df \\\n",
    "    .withColumn(\"lat_grid\", (col(\"decimalLatitude\") / grid_size).cast(\"int\") * grid_size) \\\n",
    "    .withColumn(\"lon_grid\", (col(\"decimalLongitude\") / grid_size).cast(\"int\") * grid_size) \\\n",
    "    .groupBy(\"lat_grid\", \"lon_grid\") \\\n",
    "    .agg(countDistinct(col('scientificName')).alias('species_richness'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migratory phenomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Filter out rows with missing coordinates, species, year, or temperature data\n",
    "filtered_df = df.filter(col('decimalLatitude').isNotNull() & \n",
    "                             col('decimalLongitude').isNotNull() & \n",
    "                             col('scientificName').isNotNull() & \n",
    "                             col('date_year').isNotNull() & \n",
    "                             col('sst').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify regions based on sea surface temperature (sst)\n",
    "def classify_temperature(sst):\n",
    "    if sst is None:\n",
    "        return 'unknown'\n",
    "    if sst < 10:\n",
    "        return 'cold'\n",
    "    elif 10 <= sst <= 25:\n",
    "        return 'temperate'\n",
    "    else:\n",
    "        return 'warm'\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    'sst', \n",
    "    F.when(F.col('sst').cast(DoubleType()).isNotNull(), F.col('sst').cast(DoubleType())).otherwise(None)\n",
    ")\n",
    "\n",
    "temperature_class_udf = udf(classify_temperature, StringType())\n",
    "classified_df = filtered_df.withColumn('temperature_region', temperature_class_udf(col('sst')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "# Calculate the average latitude and longitude for each species in each year\n",
    "species_movement_df = classified_df.groupBy(\"scientificName\", \"date_year\", \"temperature_region\") \\\n",
    "                                       .agg(avg(col('decimalLatitude')).alias('avg_latitude'),\n",
    "                                            avg(col('decimalLongitude')).alias('avg_longitude'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "valid_data_df = df.filter(df[\"scientificName\"].isNotNull() & df[\"date_year\"].isNotNull())\n",
    "\n",
    "# Calculate the number of species observed in each year\n",
    "species_counts_per_year = valid_data_df.groupBy(\"date_year\", \"scientificName\").count()\n",
    "species_counts_per_year = species_counts_per_year.withColumnRenamed(\"count\", \"species_count\")\n",
    "\n",
    "\n",
    "# Calculate total species count per year\n",
    "total_counts_per_year = species_counts_per_year.groupBy(\"date_year\").agg(\n",
    "    F.sum(\"species_count\").alias(\"total_count_per_year\")\n",
    ")\n",
    "\n",
    "# Join total counts back to the original species count DataFrame to calculate proportions\n",
    "species_with_proportion = species_counts_per_year.join(\n",
    "    total_counts_per_year, on=\"date_year\"\n",
    ").withColumn(\n",
    "    \"proportion\", F.col(\"species_count\") / F.col(\"total_count_per_year\")\n",
    ")\n",
    "\n",
    "# Calculate Shannon component (-p_i * ln(p_i)) for each species within each year\n",
    "species_with_shannon_component = species_with_proportion.withColumn(\n",
    "    \"shannon_component\", -F.col(\"proportion\") * F.log(F.col(\"proportion\"))\n",
    ")\n",
    "\n",
    "# Sum the Shannon components per year to get the Shannon index for each year\n",
    "shannon_indices = species_with_shannon_component.groupBy(\"date_year\").agg(\n",
    "    F.sum(\"shannon_component\").alias(\"shannon_index\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store output datas in database (MongoDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "species_count_per_species.write.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"biodiversity_db\") \\\n",
    "    .option(\"collection\", \"statistics\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "species_richness_df.write.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"biodiversity_db\") \\\n",
    "    .option(\"collection\", \"biodiversity_hotspots\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "species_movement_df.write.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"biodiversity_db\") \\\n",
    "    .option(\"collection\", \"migration\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shannon_indices.write.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"biodiversity_db\") \\\n",
    "    .option(\"collection\", \"shannon\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [localhost]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [emma-Inspiron-3501]\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/sbin/stop-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
