{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y openjdk-11-jdk-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!readlink -f $(which java) | sed \"s:bin/java::\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install ssh\n",
    "!sudo apt-get install pdsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf hadoop-3.3.6.tar.gz\n",
    "!sudo mv hadoop-3.3.6 /usr/local/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hadoop_env_path = '/usr/local/hadoop/etc/hadoop/hadoop-env.sh'\n",
    "\n",
    "with open(hadoop_env_path, 'a') as f:\n",
    "    f.write(f'\\nexport JAVA_HOME={os.environ[\"JAVA_HOME\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/bin/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure core-site.xml\n",
    "core_site_xml = \"\"\"\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "\"\"\"\n",
    "\n",
    "with open('/usr/local/hadoop/etc/hadoop/core-site.xml', 'w') as file:\n",
    "    file.write(core_site_xml)\n",
    "\n",
    "# Configure hdfs-site.xml\n",
    "hdfs_site_xml = \"\"\"\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "\"\"\"\n",
    "\n",
    "with open('/usr/local/hadoop/etc/hadoop/hdfs-site.xml', 'w') as file:\n",
    "    file.write(hdfs_site_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh localhost exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/bin/hdfs namenode -format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "pdsh@emma-Inspiron-3501: localhost: ssh exited with exit code 1\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [emma-Inspiron-3501]\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /user/OBIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -v https://dlcdn.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvf spark-3.4.3-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv spark-3.4.3-bin-hadoop3 /opt/spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install findspark --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "# Set up environment variables\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/bin/hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "if spark is None:\n",
    "    # No active session, create a new one\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"OBIS\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"New Spark session created.\")\n",
    "else:\n",
    "    # An active session already exists\n",
    "    print(\"Spark session is already running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the local CSV file\n",
    "csv_file_path = \"./obis_20230208.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to HDFS directory\n",
    "hdfs_output_path = \"hdfs://localhost:9000/user/OBIS/data\"\n",
    "\n",
    "# Write the DataFrame to HDFS\n",
    "df.write.csv(hdfs_output_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HDFS directory path (adjust as necessary)\n",
    "hdfs_directory_path = \"hdfs://localhost:9000/user/OBIS/data\"\n",
    "\n",
    "# Read files from the HDFS directory into a DataFrame\n",
    "try:\n",
    "    df = spark.read.option(\"header\", \"true\").csv(hdfs_directory_path)  # Change .csv to .parquet or .json as needed\n",
    "    # df.show()  # Display the DataFrame content\n",
    "except Exception as e:\n",
    "    print(\"Error reading from HDFS:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Stat about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, countDistinct\n",
    "\n",
    "# Compute statistics\n",
    "species_count = df.select(countDistinct(\"scientificname\")).alias(\"species_count\")\n",
    "mean_depth = df.select(mean(\"depth\")).alias(\"mean_depth\")\n",
    "stddev_depth = df.select(stddev(\"depth\")).alias(\"stddev_depth\")\n",
    "\n",
    "# Show results\n",
    "species_count.show()\n",
    "mean_depth.show()\n",
    "stddev_depth.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = species_count.collect()[0][0]\n",
    "mean = mean_depth.collect()[0][0]\n",
    "stddev = stddev_depth.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Species Count: {species}\")\n",
    "print(f\"Mean Depth: {mean}\")\n",
    "print(f\"Standard Deviation of Depth: {stddev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of biodiversity hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "cleaned_df = df.filter(col('decimalLatitude').isNotNull() & col('decimalLongitude').isNotNull() & col('scientificName').isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "grid_size = 1.0\n",
    "species_richness_df = cleaned_df \\\n",
    "    .withColumn(\"lat_grid\", (col(\"decimalLatitude\") / grid_size).cast(\"int\") * grid_size) \\\n",
    "    .withColumn(\"lon_grid\", (col(\"decimalLongitude\") / grid_size).cast(\"int\") * grid_size) \\\n",
    "    .groupBy(\"lat_grid\", \"lon_grid\") \\\n",
    "    .agg(countDistinct(col('scientificName')).alias('species_richness'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = species_richness_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migratory phenomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Filter out rows with missing coordinates, species, year, or temperature data\n",
    "filtered_df = df.filter(col('decimalLatitude').isNotNull() & \n",
    "                             col('decimalLongitude').isNotNull() & \n",
    "                             col('scientificName').isNotNull() & \n",
    "                             col('date_year').isNotNull() & \n",
    "                             col('sst').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify regions based on sea surface temperature (sst)\n",
    "def classify_temperature(sst):\n",
    "    if sst < 10:\n",
    "        return 'cold'\n",
    "    elif 10 <= sst <= 25:\n",
    "        return 'temperate'\n",
    "    else:\n",
    "        return 'warm'\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "temperature_class_udf = udf(classify_temperature, StringType())\n",
    "classified_df = filtered_df.withColumn('temperature_region', temperature_class_udf(col('sst')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "# Calculate the average latitude and longitude for each species in each year\n",
    "species_movement_df = classified_df.groupBy(\"scientificName\", \"date_year\") \\\n",
    "                                   .agg(avg(col('decimalLatitude')).alias('avg_latitude'),\n",
    "                                        avg(col('decimalLongitude')).alias('avg_longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_movement_pd = species_movement_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Database (MongoDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pymongo --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "mongo_uri = \"mongodb+srv://obis:obis-project@obis-results.cer3o.mongodb.net/?retryWrites=true&w=majority&appName=OBIS-results\"\n",
    "client = MongoClient(mongo_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client[\"biodiversity_db\"]\n",
    "collection = db[\"statistics\"]\n",
    "\n",
    "# Create a document with the statistics\n",
    "stats_document = {\n",
    "    \"species_count\": species,\n",
    "    \"mean_depth\": mean,\n",
    "    \"stddev_depth\": stddev\n",
    "}\n",
    "\n",
    "# Insert the document into the collection\n",
    "collection.insert_one(stats_document)\n",
    "print(\"Statistics stored in MongoDB successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert biodiversity hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_data = pandas_df[['lat_grid', 'lon_grid', 'species_richness']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb+srv://obis:obis-project@obis-results.cer3o.mongodb.net/?retryWrites=true&w=majority&appName=OBIS-results\")\n",
    "db = client['biodiversity_db'] \n",
    "collection = db['biodiversity_hotspots']\n",
    "\n",
    "# Insert hotspots data into MongoDB\n",
    "collection.insert_many(hotspots_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_data = species_movement_pd.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb+srv://obis:obis-project@obis-results.cer3o.mongodb.net/?retryWrites=true&w=majority&appName=OBIS-results\")\n",
    "db = client['biodiversity_db'] \n",
    "collection = db['migration']\n",
    "\n",
    "# Insert hotspots data into MongoDB\n",
    "collection.insert_many(migration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [localhost]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [emma-Inspiron-3501]\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
