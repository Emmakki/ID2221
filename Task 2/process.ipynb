{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/bin/hadoop fs -ls /user/OBIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def init_spark():\n",
    "    mongo_conn = \"mongodb+srv://obis:obis-project@obis-results.cer3o.mongodb.net/?retryWrites=true&w=majority&appName=OBIS-results\"\n",
    "    conf = SparkConf()\n",
    "\n",
    "    # Download mongo-spark-connector and its dependencies.\n",
    "    conf.set(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\")\n",
    "\n",
    "    # Set up read connection :\n",
    "    conf.set(\"spark.mongodb.read.connection.uri\", mongo_conn)\n",
    "    conf.set(\"spark.mongodb.read.database\", \"biodiversity_db\")\n",
    "\n",
    "    # Set up write connection\n",
    "    conf.set(\"spark.mongodb.write.connection.uri\", mongo_conn)\n",
    "    conf.set(\"spark.mongodb.write.database\", \"biodiversity_db\")\n",
    "    conf.set(\"spark.mongodb.write.operationType\", \"update\")\n",
    "\n",
    "    SparkContext(conf=conf)\n",
    "\n",
    "    return SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('OBIS') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_directory_path = \"hdfs://localhost:9000/user/OBIS/data\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.option(\"header\", \"true\").csv(hdfs_directory_path)\n",
    "except Exception as e:\n",
    "    print(\"Error reading from HDFS:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "species_count_per_species = df.groupBy(\"scientificname\").agg(count(\"*\").alias(\"count_per_species\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biodiversity hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "cleaned_df = df.filter(col('decimalLatitude').isNotNull() & col('decimalLongitude').isNotNull() & col('scientificName').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "grid_size = 1.0\n",
    "species_richness_df = cleaned_df \\\n",
    "    .withColumn(\"lat_grid\", (col(\"decimalLatitude\") / grid_size).cast(\"int\") * grid_size) \\\n",
    "    .withColumn(\"lon_grid\", (col(\"decimalLongitude\") / grid_size).cast(\"int\") * grid_size) \\\n",
    "    .groupBy(\"lat_grid\", \"lon_grid\") \\\n",
    "    .agg(countDistinct(col('scientificName')).alias('species_richness'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migratory phenomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Filter out rows with missing coordinates, species, year, or temperature data\n",
    "filtered_df = df.filter(col('decimalLatitude').isNotNull() & \n",
    "                             col('decimalLongitude').isNotNull() & \n",
    "                             col('scientificName').isNotNull() & \n",
    "                             col('date_year').isNotNull() & \n",
    "                             col('sst').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify regions based on sea surface temperature (sst)\n",
    "def classify_temperature(sst):\n",
    "    if sst is None:\n",
    "        return 'unknown'\n",
    "    if sst < 10:\n",
    "        return 'cold'\n",
    "    elif 10 <= sst <= 25:\n",
    "        return 'temperate'\n",
    "    else:\n",
    "        return 'warm'\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    'sst', \n",
    "    F.when(F.col('sst').cast(DoubleType()).isNotNull(), F.col('sst').cast(DoubleType())).otherwise(None)\n",
    ")\n",
    "\n",
    "temperature_class_udf = udf(classify_temperature, StringType())\n",
    "classified_df = filtered_df.withColumn('temperature_region', temperature_class_udf(col('sst')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "# Calculate the average latitude and longitude for each species in each year\n",
    "species_movement_df = classified_df.groupBy(\"scientificName\", \"date_year\", \"temperature_region\") \\\n",
    "                                       .agg(avg(col('decimalLatitude')).alias('avg_latitude'),\n",
    "                                            avg(col('decimalLongitude')).alias('avg_longitude'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "valid_data_df = df.filter(df[\"scientificName\"].isNotNull() & df[\"date_year\"].isNotNull())\n",
    "\n",
    "# Calculate the number of species observed in each year\n",
    "species_counts_per_year = valid_data_df.groupBy(\"date_year\", \"scientificName\").count()\n",
    "species_counts_per_year = species_counts_per_year.withColumnRenamed(\"count\", \"species_count\")\n",
    "\n",
    "\n",
    "# Calculate total species count per year\n",
    "total_counts_per_year = species_counts_per_year.groupBy(\"date_year\").agg(\n",
    "    F.sum(\"species_count\").alias(\"total_count_per_year\")\n",
    ")\n",
    "\n",
    "# Join total counts back to the original species count DataFrame to calculate proportions\n",
    "species_with_proportion = species_counts_per_year.join(\n",
    "    total_counts_per_year, on=\"date_year\"\n",
    ").withColumn(\n",
    "    \"proportion\", F.col(\"species_count\") / F.col(\"total_count_per_year\")\n",
    ")\n",
    "\n",
    "# Calculate Shannon component (-p_i * ln(p_i)) for each species within each year\n",
    "species_with_shannon_component = species_with_proportion.withColumn(\n",
    "    \"shannon_component\", -F.col(\"proportion\") * F.log(F.col(\"proportion\"))\n",
    ")\n",
    "\n",
    "# Sum the Shannon components per year to get the Shannon index for each year\n",
    "shannon_indices = species_with_shannon_component.groupBy(\"date_year\").agg(\n",
    "    F.sum(\"shannon_component\").alias(\"shannon_index\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "grid_size = 1.0  \n",
    "\n",
    "# spatial and temporal coverage\n",
    "coverage_df = df \\\n",
    "    .filter(F.col(\"decimalLatitude\").isNotNull() & \n",
    "            F.col(\"decimalLongitude\").isNotNull() & \n",
    "            F.col(\"date_year\").isNotNull()) \\\n",
    "    .withColumn(\"lat_grid\", (F.col(\"decimalLatitude\") / grid_size).cast(\"int\") * grid_size) \\\n",
    "    .withColumn(\"lon_grid\", (F.col(\"decimalLongitude\") / grid_size).cast(\"int\") * grid_size) \\\n",
    "    .groupBy(\"lat_grid\", \"lon_grid\", \"date_year\") \\\n",
    "    .agg(F.count(\"*\").alias(\"record_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store output datas in database (MongoDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_count_per_species.write.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"biodiversity_db\") \\\n",
    "    .option(\"collection\", \"statistics\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_richness_df.write.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"biodiversity_db\") \\\n",
    "    .option(\"collection\", \"biodiversity_hotspots\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_movement_df.write.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"biodiversity_db\") \\\n",
    "    .option(\"collection\", \"migration\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shannon_indices.write.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"biodiversity_db\") \\\n",
    "    .option(\"collection\", \"shannon\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df.write.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"biodiversity_db\") \\\n",
    "    .option(\"collection\", \"data_coverage\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/sbin/stop-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
