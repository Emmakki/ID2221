{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install openjdk-11-jdk -y\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://downloads.apache.org/hbase/2.5.10/hbase-2.5.10-bin.tar.gz\n",
    "!tar xvf hbase-2.5.10-bin.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mv hbase-2.5.10 /opt/hbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update HBase environment configuration\n",
    "!sed -i 's|# export JAVA_HOME=.*|export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64|' /opt/hbase/conf/hbase-env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify JAVA_HOME and set if necessary\\n\",\n",
    "import os\n",
    "\n",
    "# Check if JAVA_HOME is set, if not set it to the correct path\\n\",\n",
    "java_home = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "if not os.path.exists(java_home):\n",
    "    raise FileNotFoundError(f'Java path not found: {java_home}')\n",
    "\n",
    "# Set JAVA_HOME and update PATH\\n\",\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "os.environ['PATH'] = java_home + '/bin:' + os.environ['PATH']\n",
    "\n",
    "# Verify the java version to confirm setup\\n\",\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start HBase\n",
    "import os\n",
    "\n",
    "original_directory = os.getcwd()\n",
    "\n",
    "os.chdir('/opt/hbase')\n",
    "!bin/start-hbase.sh\n",
    "\n",
    "os.chdir(original_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HBase Table\n",
    "hbase_commands = '''\n",
    "create 'my_table', 'cf1'\n",
    "list\n",
    "exit\n",
    "'''\n",
    "\n",
    "# Pass the commands to the HBase shell\n",
    "with open('create_table.txt', 'w') as file:\n",
    "    file.write(hbase_commands)\n",
    "\n",
    "!cat create_table.txt | /opt/hbase/bin/hbase shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/emma/Documents/KTH/ID2221/ID2221/Task 2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dlcdn.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvf spark-3.4.3-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mv spark-3.4.3-bin-hadoop3 /opt/spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Set the SPARK_HOME environment variable\n",
    "spark_home = '/opt/spark'\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "\n",
    "# Update the PATH to include Spark's bin directory\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['SPARK_HOME'], 'bin')\n",
    "\n",
    "# Find all .zip files in the Spark python/lib directory and set PYTHONPATH\n",
    "zip_files = glob.glob(os.path.join(spark_home, 'python', 'lib', '*.zip'))\n",
    "new_pythonpath = ':'.join(zip_files)\n",
    "os.environ['PYTHONPATH'] = f\"{new_pythonpath}:{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "# Print environment variables to verify\n",
    "print(\"SPARK_HOME:\", os.environ['SPARK_HOME'])\n",
    "print(\"PYTHONPATH:\", os.environ['PYTHONPATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('PySparkTest') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Spark session\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('HBaseReader') \\\n",
    "    .config('spark.hadoop.hbase.zookeeper.quorum', 'localhost') \\\n",
    "    .config('spark.hadoop.hbase.master', 'localhost:16000') \\\n",
    "    .config('spark.hadoop.hbase.spark.sql.hbase.connection', 'localhost:2181') \\\n",
    "    .config('spark.hadoop.hbase.table', 'my_table') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the HBase table\n",
    "df = spark.read.format('org.apache.hadoop.hbase.spark') \\\n",
    "    .option('hbase.table', 'my_table') \\\n",
    "    .option('hbase.zookeeper.quorum', 'localhost') \\\n",
    "    .option('hbase.zookeeper.property.clientPort', '2181') \\\n",
    "    .load()\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "original_directory = os.getcwd()\n",
    "\n",
    "os.chdir('/opt/hbase')\n",
    "!bin/stop-hbase.sh\n",
    "\n",
    "os.chdir(original_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
