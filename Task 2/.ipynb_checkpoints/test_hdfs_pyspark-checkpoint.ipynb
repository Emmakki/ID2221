{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y openjdk-11-jdk-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!readlink -f $(which java) | sed \"s:bin/java::\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install ssh\n",
    "!sudo apt-get install pdsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf hadoop-3.3.6.tar.gz\n",
    "!sudo mv hadoop-3.3.6 /usr/local/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hadoop_env_path = '/usr/local/hadoop/etc/hadoop/hadoop-env.sh'\n",
    "\n",
    "with open(hadoop_env_path, 'a') as f:\n",
    "    f.write(f'\\nexport JAVA_HOME={os.environ[\"JAVA_HOME\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/bin/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure core-site.xml\n",
    "core_site_xml = \"\"\"\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "\"\"\"\n",
    "\n",
    "with open('/usr/local/hadoop/etc/hadoop/core-site.xml', 'w') as file:\n",
    "    file.write(core_site_xml)\n",
    "\n",
    "# Configure hdfs-site.xml\n",
    "hdfs_site_xml = \"\"\"\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "\"\"\"\n",
    "\n",
    "with open('/usr/local/hadoop/etc/hadoop/hdfs-site.xml', 'w') as file:\n",
    "    file.write(hdfs_site_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh localhost exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/bin/hdfs namenode -format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [emma-Inspiron-3501]\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113157 SecondaryNameNode\r\n",
      "112679 NameNode\r\n",
      "113286 Jps\r\n",
      "54456 SparkSubmit\r\n",
      "112895 DataNode\r\n"
     ]
    }
   ],
   "source": [
    "!sudo jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /user/OBIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -v https://dlcdn.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvf spark-3.4.3-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv spark-3.4.3-bin-hadoop3 /opt/spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install findspark --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "# Set up environment variables\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/hadoop/bin/hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "if spark is None:\n",
    "    # No active session, create a new one\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"OBIS\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"New Spark session created.\")\n",
    "else:\n",
    "    # An active session already exists\n",
    "    print(\"Spark session is already running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the local CSV file\n",
    "csv_file_path = \"./obis_20230208.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to HDFS directory\n",
    "hdfs_output_path = \"hdfs://localhost:9000/user/OBIS/data\"\n",
    "\n",
    "# Write the DataFrame to HDFS\n",
    "df.write.csv(hdfs_output_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HDFS directory path (adjust as necessary)\n",
    "hdfs_directory_path = \"hdfs://localhost:9000/user/OBIS/data\"\n",
    "\n",
    "# Read files from the HDFS directory into a DataFrame\n",
    "try:\n",
    "    df = spark.read.option(\"header\", \"true\").csv(hdfs_directory_path)  # Change .csv to .parquet or .json as needed\n",
    "    df.show()  # Display the DataFrame content\n",
    "except Exception as e:\n",
    "    print(\"Error reading from HDFS:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Stat about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, countDistinct\n",
    "\n",
    "# Compute statistics\n",
    "species_count = df.select(countDistinct(\"scientificname\")).alias(\"species_count\")\n",
    "mean_depth = df.select(mean(\"depth\")).alias(\"mean_depth\")\n",
    "stddev_depth = df.select(stddev(\"depth\")).alias(\"stddev_depth\")\n",
    "\n",
    "# Show results\n",
    "species_count.show()\n",
    "mean_depth.show()\n",
    "stddev_depth.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = species_count.collect()[0][0]\n",
    "mean = mean_depth.collect()[0][0]\n",
    "stddev = stddev_depth.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Species Count: {species}\")\n",
    "print(f\"Mean Depth: {mean}\")\n",
    "print(f\"Standard Deviation of Depth: {stddev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map the spatial distribution of species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round, count\n",
    "\n",
    "# Round latitude and longitude to 2 decimal places for spatial resolution\n",
    "df = df.withColumn('latitude_rounded', round(df['decimalLatitude'], 2)) \\\n",
    "       .withColumn('longitude_rounded', round(df['decimalLongitude'], 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by species, rounded latitude, and rounded longitude\n",
    "df_grouped = df.groupBy('scientificName', 'latitude_rounded', 'longitude_rounded') \\\n",
    "               .agg(count('*').alias('observation_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Database (MongoDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pymongo --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "mongo_uri = \"mongodb+srv://obis:obis-project@obis-results.cer3o.mongodb.net/?retryWrites=true&w=majority&appName=OBIS-results\"\n",
    "client = MongoClient(mongo_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client[\"biodiversity_db\"]\n",
    "collection = db[\"statistics\"]\n",
    "\n",
    "# Create a document with the statistics\n",
    "stats_document = {\n",
    "    \"species_count\": species,\n",
    "    \"mean_depth\": mean,\n",
    "    \"stddev_depth\": stddev\n",
    "}\n",
    "\n",
    "# Insert the document into the collection\n",
    "collection.insert_one(stats_document)\n",
    "print(\"Statistics stored in MongoDB successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert spacial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the grouped DataFrame to JSON and then to a list of dictionaries\n",
    "json_records = df_grouped.toJSON().collect()\n",
    "records = [json.loads(record) for record in json_records]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db['distribution_data']\n",
    "\n",
    "# Insert the processed data into MongoDB\n",
    "collection.insert_many(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [localhost]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [emma-Inspiron-3501]\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
